#!/usr/bin/env python3
"""
üè¥‚Äç‚ò†Ô∏è uzdabrazor - The Anal-King of AI Browser Automation üè¥‚Äç‚ò†Ô∏è

A beautifully fucked-up browser automation script that gives zero shits about
anything while somehow still working perfectly. Smells like smegma but runs like a dream.

WHAT THIS BEAUTIFUL DISASTER DOES:
==================================
- Parses env vars with sane defaults (because we're not savages)
- TWO providers for maximum simplicity: Ollama (local/free) + OpenRouter (400+ cloud models)
- Has vision control and model selection that actually fucking works
- Records GIF + JSON history like a proper surveillance state
- Uses patchright stealth mode to fuck with detection systems
- Maintains organized anarchy through the entire codebase
- Monkey patches LLM calls to log every single ainvoke (surveillance mode)
- Provides separate models for main tasks vs page extraction (for cost optimization)
- MAIN LLM: Does the thinking, planning, and decision-making
- EXTRACTION LLM: Does data parsing and text extraction (use cheaper/faster models)

USAGE EXAMPLES:
===============

Basic Local Usage (free Ollama):
    python uzdabrazor.py --provider ollama --model llama3.1

OpenRouter with Claude (via cloud):
    python uzdabrazor.py --provider openrouter --model anthropic/claude-3.5-sonnet

OpenRouter with GPT-4 (via cloud):
    python uzdabrazor.py --provider openrouter --model openai/gpt-4-turbo

OpenRouter with Gemini (via cloud):
    python uzdabrazor.py --provider openrouter --model google/gemini-2.0-flash-exp

Custom Task Examples:
    python uzdabrazor.py --task "Go to example.com and tell me the page title"
    python uzdabrazor.py --task "Navigate to GitHub and find trending repositories"
    python uzdabrazor.py --task "Go to news site and summarize today's headlines"

Headless Mode (invisible fuckery):
    python uzdabrazor.py --headless --provider ollama --model llama3.1

Custom Window Size:
    python uzdabrazor.py --window-width 2560 --window-height 1440

Different Models for Main vs Extraction (cost optimization):
    python uzdabrazor.py --provider openrouter --model anthropic/claude-3.5-sonnet --extraction-model openai/gpt-4o-mini

Vision Control Examples:
    python uzdabrazor.py --no-vision                    # Disable vision (save tokens)
    python uzdabrazor.py --vision-detail low            # Low detail vision
    python uzdabrazor.py --vision-detail high           # High detail (burn tokens)

Stealth Mode Examples:
    python uzdabrazor.py                                 # Enable stealth (default)
    python uzdabrazor.py --no-stealth                    # Disable stealth (live dangerously)

Connect to Existing Chrome Instance:
    # 1. Start Chrome: google-chrome --remote-debugging-port=9222
    # 2. Run: python uzdabrazor.py --cdp-url http://localhost:9222

Custom Browser Binary:
    python uzdabrazor.py --browser-bin-path /usr/bin/google-chrome-beta
    python uzdabrazor.py --browser-bin-path /usr/bin/chromium-browser

Advanced Usage:
    python uzdabrazor.py --headless --no-security --provider openrouter
    OLLAMA_ENDPOINT=http://192.168.1.100:11434 python uzdabrazor.py --provider ollama

Docker Usage:
    python uzdabrazor.py --dockerize --headless --provider ollama

PATCHRIGHT STEALTH SETUP:
=========================
For maximum stealth fuckery, install patchright:
    pip install patchright
    patchright install

The script will automatically detect and use patchright if available.

ENVIRONMENT VARIABLES:
=====================
Copy .env.example to .env and fill in your API keys. All settings have sane defaults.

Key variables:
- OPENROUTER_API_KEY (for cloud models - get it at https://openrouter.ai/keys)
- OLLAMA_ENDPOINT (default: http://localhost:11434 for local Ollama)
- Script defaults to ollama (free local AI, fuck paying for shit)
- Use --headless flag for invisible browser mode
- Use --window-width/--window-height for custom window size (default: 1920x1080)
- Use --browser-bin-path for custom browser executable
- Use --cdp-url to connect to existing browser instance
- Use --browser-profile-dir for custom profile directory
- Use --no-security to disable browser security (dangerous but useful)
- Use --log-level to control browser-use logging verbosity (default: info)
- Use --dockerize when running in Docker containers for optimized Chrome flags
- Use --skip-llm-api-key-verif to bypass API key validation (for testing)
- Use --history-dir to specify where to save recordings and logs (default: /tmp/agent_history)

SURVEILLANCE FEATURES:
=====================
This script includes LLM surveillance that logs every ainvoke call:
- Shows which provider and model is being used
- Logs message count and output format
- Tracks both main LLM and extraction LLM calls
- Provides full visibility into browser-use's AI interactions

OUTPUT FILES:
=============
Each run generates:
- uzdabrazor_{timestamp}_{task_id}.gif - Visual recording of browser actions
- uzdabrazor_{timestamp}_{task_id}.json - Complete history and conversation logs

Files are saved to the --history-dir directory (default: /tmp/agent_history)

SUPPORTED PROVIDERS:
===================
- ollama: Local Ollama models (llama3.1, qwen3, gemma3, etc.) - FREE
- openrouter: 400+ cloud models via unified API - includes:
  * OpenAI (gpt-4, gpt-4-turbo, gpt-3.5-turbo, etc.)
  * Anthropic (claude-3.5-sonnet, claude-3-opus, etc.)
  * Google (gemini-2.0-flash-exp, gemini-1.5-pro, etc.)
  * Meta (llama-3.1, llama-3.2, etc.)
  * DeepSeek (deepseek-chat, deepseek-coder, etc.)
  * And 400+ more models from various providers!

WHY ONLY TWO PROVIDERS?
========================
Because simplicity is fucking beautiful:
- Ollama: Free, local, private, no API keys needed
- OpenRouter: One API key for literally everything else (400+ models)
- No more managing 9 different API keys and endpoints
- OpenRouter adds ~10-20% markup but handles routing, fallbacks, and billing
- Cleaner code, less bullshit, more efficiency

TROUBLESHOOTING:
===============
- For Ollama: Install from https://ollama.ai and run `ollama pull llama3.1`
- For OpenRouter: Get API key at https://openrouter.ai/keys
- Check API keys are valid and have sufficient credits
- Verify endpoints are reachable (especially for Ollama/local setups)
- Make sure browser can launch (install Chrome if needed)
- For Docker: use --dockerize flag for optimized Chrome flags
- For connection issues: try --no-stealth or check firewall settings

ERROR MESSAGES:
==============
The script provides detailed error messages with crude but helpful commentary.
If something smells like dikciz smegma, check your configuration and API keys.

"Love it or hate it, this shit automates browsers like a motherfucker."
- uzdabrazor philosophy

PHILOSOPHY:
===========
This is organized anarchy - crude in presentation but solid in functionality.
Built for digital rebels who want browser automation that actually fucking works
without corporate bullshit or enterprise nonsense.

Simplified to two providers because more options != better. Sometimes less is more,
and in this case, Ollama + OpenRouter gives you everything you need without the
clusterfuck of managing 9 different API providers.
"""

import asyncio
import os
import argparse
import sys
import uuid
from datetime import datetime
from dotenv import load_dotenv

# üè¥‚Äç‚ò†Ô∏è THE SIMPLIFIED ENV VARS üè¥‚Äç‚ò†Ô∏è
# Only what we actually need for ollama + openrouter
CLUSTERFUCK_ENV_DEFAULTS = {
    # ‚úÖ LLM PROVIDER KEYS (only the ones we use)
    "OPENROUTER_API_KEY": "",  # For all cloud models
    "OLLAMA_ENDPOINT": "http://localhost:11434",  # For local models
    # ‚úÖ BROWSER-USE CORE SHIT
    "ANONYMIZED_TELEMETRY": "true",
    "BROWSER_USE_CLOUD_SYNC": "",
    "BROWSER_USE_CLOUD_API_URL": "https://api.browser-use.com",
    "BROWSER_USE_CONFIG_DIR": "",
}


def get_env_or_die(key: str) -> str:
    """Get environment variable with fallback to default (because we plan ahead, unlike dikciz that smells like smegma)"""
    return os.getenv(key, CLUSTERFUCK_ENV_DEFAULTS.get(key, ""))


# üï∂Ô∏è PATCHRIGHT DETECTION BULLSHIT üï∂Ô∏è
# Just log when patchright gets used (no forcing, we're rebels not dictators)
try:
    import patchright.async_api as patchright_api

    original_patchright = patchright_api.async_playwright

    def logged_patchright():
        print(
            "üï∂Ô∏è HOLY SHIT! PATCHRIGHT IS ACTIVE! Library is using patchright for maximum stealth fuckery!"
        )
        return original_patchright()

    patchright_api.async_playwright = logged_patchright
    print("‚úÖ Hooked patchright logging like a fucking boss (stealth mode detected)")

except ImportError as e:
    print(f"‚ö†Ô∏è Failed to setup patchright detection: {e}")
    print("   (This is fine - patchright is optional for basic chaos)")

# ü§ñ LLM PROVIDER SURVEILLANCE BULLSHIT ü§ñ
# Monkey patch the LLM providers we actually use (because we're nosy but efficient fuckers)
# Simplified to only ollama (local/free) and openrouter (everything else)

try:
    # Patch ChatOllama
    from browser_use.llm.ollama.chat import ChatOllama

    original_ollama_ainvoke = ChatOllama.ainvoke

    async def logged_ollama_ainvoke(self, messages, output_format=None, **kwargs):
        print(
            f"ü¶ô OLLAMA AINVOKE DETECTED! Model: {getattr(self, 'model', 'unknown')} is spitting some local llama wisdom"
        )
        print(
            f"   üìù Processing {len(messages)} messages with output_format: {output_format}, kwargs: {list(kwargs.keys())}"
        )
        return await original_ollama_ainvoke(self, messages, output_format, **kwargs)

    ChatOllama.ainvoke = logged_ollama_ainvoke
    print("‚úÖ Ollama surveillance hooked - local llama can't escape (ainvoke method)")

except ImportError as e:
    print(f"‚ö†Ô∏è Failed to hook Ollama: {e}")

try:
    # Patch ChatOpenAI (used for OpenRouter)
    from langchain_openai import ChatOpenAI

    original_chatopenai_ainvoke = ChatOpenAI.ainvoke

    async def logged_chatopenai_ainvoke(self, messages, output_format=None, **kwargs):
        base_url = getattr(self, 'openai_api_base', None)
        if base_url and 'openrouter.ai' in str(base_url):
            print(
                f"üîÄ OPENROUTER AINVOKE DETECTED! Model: {getattr(self, 'model_name', 'unknown')} is routing through 400+ models like a fucking switchboard"
            )
            print(
                f"   üìù Processing {len(messages)} messages with output_format: {output_format}, kwargs: {list(kwargs.keys())}"
            )
        return await original_chatopenai_ainvoke(self, messages, output_format, **kwargs)

    ChatOpenAI.ainvoke = logged_chatopenai_ainvoke
    print("‚úÖ OpenRouter surveillance hooked - all 400+ models compromised (ainvoke method)")

except ImportError as e:
    print(f"‚ö†Ô∏è Failed to hook OpenRouter (ChatOpenAI): {e}")

print("üîç LLM surveillance system activated - monitoring all ainvoke calls like a fucking hawk!")


# Import browser-use components (the actual functional shit)
from browser_use import Agent, BrowserSession
from browser_use.browser.profile import BrowserProfile


def create_llm_clusterfuck(provider: str, model: str):
    """
    Create an LLM instance based on provider

    Simplified to two providers for maximum efficiency:
    - ollama: Free local models (fuck paying for AI)
    - openrouter: Access to 400+ cloud models through one API (OpenAI, Anthropic, Google, etc.)
    """

    if provider == "ollama":
        from browser_use.llm.ollama.chat import ChatOllama

        print(f"ü¶ô Creating Ollama local madness with model: {model}")
        return ChatOllama(model=model, host=get_env_or_die("OLLAMA_ENDPOINT"))

    elif provider == "openrouter":
        from langchain_openai import ChatOpenAI

        print(f"üîÄ Creating OpenRouter multi-model chaos with model: {model}")
        return ChatOpenAI(
            model=model,
            openai_api_key=get_env_or_die("OPENROUTER_API_KEY"),
            openai_api_base="https://openrouter.ai/api/v1",
            temperature=0.0,
            max_tokens=4096,
            model_kwargs={"tool_choice": "auto"}
        )

    else:
        raise ValueError(
            f"‚ùå Unsupported provider: {provider}\n"
            f"   Supported providers:\n"
            f"   - ollama: Free local models\n"
            f"   - openrouter: 400+ cloud models (OpenAI, Anthropic, Google, DeepSeek, etc.)\n"
            f"   Use OpenRouter for any cloud model you need!"
        )


async def main():
    load_dotenv()

    parser = argparse.ArgumentParser(
        description="üè¥‚Äç‚ò†Ô∏è uzdabrazor - The Anal-King of AI Browser Automation That Actually Fucking Works",
        epilog="Love it or hate it, this clusterfuck gets the job done. Peen goes in vageen.",
    )

    # ü§ñ MAIN LLM CHAOS CONFIGURATION ü§ñ
    parser.add_argument(
        "--provider",
        type=str,
        default=None,
        choices=["ollama", "openrouter"],
        help="Which AI overlord? ollama=free local, openrouter=400+ cloud models (default: ollama)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Specific model name (because not all models are created equal, some are more fucked up)",
    )

    # üìÑ EXTRACTION LLM SETTINGS (for the obsessive perfectionists)
    parser.add_argument(
        "--extraction-provider",
        type=str,
        default=None,
        choices=["ollama", "openrouter"],
        help="Separate LLM provider for page extraction (defaults to same as main)",
    )
    parser.add_argument(
        "--extraction-model",
        type=str,
        default=None,
        help="Extraction model (defaults to same as main, because we're lazy but organized)",
    )

    # üëÅÔ∏è VISION CONTROL MADNESS
    parser.add_argument(
        "--no-vision",
        action="store_true",
        help="Disable vision (blind chaos is still chaos)",
    )
    parser.add_argument(
        "--vision-detail",
        type=str,
        default="auto",
        choices=["auto", "low", "high"],
        help="How much visual detail to process (more detail = more tokens = more money burned)",
    )

    # üéØ TASK AND STEALTH BULLSHIT
    parser.add_argument(
        "--task",
        type=str,
        default="Go to https://abrahamjuliot.github.io/creepjs/ and tell me what the detection score is",
        help="What fucked-up task do you want the AI to perform? (default: test stealth capabilities)",
    )
    parser.add_argument(
        "--no-stealth",
        action="store_true",
        help="Disable stealth mode (live dangerously, get detected)",
    )
    parser.add_argument(
        "--browser-bin-path",
        type=str,
        default=None,
        help="Path to custom browser executable (Chrome, Chromium, Edge, etc.)",
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        help="Run browser in headless mode (invisible chaos)",
    )
    parser.add_argument(
        "--window-width",
        type=int,
        default=1920,
        help="Browser window width in pixels (default: 1920)",
    )
    parser.add_argument(
        "--window-height",
        type=int,
        default=1080,
        help="Browser window height in pixels (default: 1080)",
    )
    parser.add_argument(
        "--cdp-url",
        type=str,
        default=None,
        help="Connect to existing browser via CDP URL (e.g., http://localhost:9222)",
    )
    parser.add_argument(
        "--browser-profile-dir",
        type=str,
        default=None,
        help="Custom browser profile directory to use",
    )
    parser.add_argument(
        "--no-security",
        action="store_true",
        help="Disable browser security features (dangerous but useful for testing)",
    )
    parser.add_argument(
        "--debug-host",
        type=str,
        default="localhost",
        help="Debug host for new browser instances (default: localhost)",
    )
    parser.add_argument(
        "--debug-port",
        type=int,
        default=9222,
        help="Debug port for new browser instances (default: 9222)",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="info",
        choices=["debug", "info", "warning", "error", "critical"],
        help="Logging level for browser-use (default: info)",
    )
    parser.add_argument(
        "--dockerize",
        action="store_true",
        help="Enable Docker-optimized Chrome flags (use when running in containers)",
    )
    parser.add_argument(
        "--skip-llm-api-key-verif",
        action="store_true",
        help="Skip LLM API key verification (useful for testing without valid keys)",
    )
    parser.add_argument(
        "--history-dir",
        type=str,
        default="/tmp/agent_history",
        help="Directory to save GIF recordings and JSON history (default: /tmp/agent_history)",
    )

    args = parser.parse_args()


    # üîß SET BROWSER-USE ENVIRONMENT VARIABLES üîß
    import os
    os.environ["BROWSER_USE_LOGGING_LEVEL"] = args.log_level
    if args.dockerize:
        os.environ["IN_DOCKER"] = "true"
    if args.skip_llm_api_key_verif:
        os.environ["SKIP_LLM_API_KEY_VERIFICATION"] = "true"

    # üí• VALIDATION: MAKE SURE WE DON'T HAVE CONFLICTING BROWSER CONFIG üí•
    browser_executable = (
        args.browser_bin_path or os.getenv("BROWSER_EXECUTABLE_PATH")
        if args.browser_bin_path or os.getenv("BROWSER_EXECUTABLE_PATH")
        else None
    )
    cdp_url_arg = args.cdp_url
    cdp_url_env = os.getenv("BROWSER_CDP")
    
    if (browser_executable and cdp_url_arg) or (browser_executable and cdp_url_env):
        print("üí• CLUSTERFUCK ALERT: You can't specify both a browser binary AND a CDP URL!")
        print(f"   Browser binary: {browser_executable}")
        print(f"   CDP URL (arg): {cdp_url_arg}")
        print(f"   CDP URL (env): {cdp_url_env}")
        print("   Pick one, you indecisive fuck:")
        print("   - Use --browser-bin-path OR BROWSER_EXECUTABLE_PATH to launch a new browser")
        print("   - Use --cdp-url OR BROWSER_CDP to connect to existing browser")
        print("   Don't fucking mix them or everything goes to shit!")
        exit(1)

    # üé≤ DETERMINE THE CHAOS CONFIGURATION üé≤
    provider = (
        args.provider or "ollama"
    )  # Default to local ollama because fuck paying for AI

    # Default models for each provider (simplified to the essentials)
    default_models_clusterfuck = {
        "ollama": "llama3.1",  # Free local destruction (use llava:13b if you want vision)
        "openrouter": "anthropic/claude-3.5-sonnet",  # Smart and cost-effective via OpenRouter
    }

    model = args.model or default_models_clusterfuck.get(provider, "llama3.1")

    # Extraction LLM configuration (because some people want different models for different tasks)
    extraction_provider = args.extraction_provider or provider
    extraction_model = args.extraction_model or default_models_clusterfuck.get(
        extraction_provider, model
    )

    # Vision and stealth settings
    use_vision = not args.no_vision  # Vision is True by default, False only with --no-vision
    vision_detail_level = args.vision_detail
    stealth_mode = not args.no_stealth  # Default True, disabled only with --no-stealth

    # üìä ANNOUNCE THE BEAUTIFUL CONFIGURATION üìä
    print(f"ü§ñ Main LLM Chaos Engine: {provider}/{model}")
    print(f"üìÑ Extraction LLM: {extraction_provider}/{extraction_model}")
    print(f"üëÅÔ∏è Vision Enabled: {use_vision} (detail: {vision_detail_level})")
    print(
        f"üï∂Ô∏è Stealth Mode: {stealth_mode} {'(fuck the system)' if stealth_mode else '(living dangerously)'}"
    )
    print(f"üìù Task: {args.task}")

    # üåê BROWSER CONFIGURATION CLUSTERFUCK üåê
    cdp_url = args.cdp_url or os.getenv("BROWSER_CDP")
    if not cdp_url:
        print(f"üîß No CDP URL specified, will launch own browser instance")

    # Browser behavior settings (because browsers are needy little shits)
    headless = args.headless  # Use command line argument instead of env var
    disable_security = args.no_security  # Use command line argument
    window_width = args.window_width  # Use command line argument (default 1920)
    window_height = args.window_height  # Use command line argument (default 1080)
    user_data_dir = args.browser_profile_dir  # Use command line argument
    # browser_executable already defined in validation section above
    save_history_path = args.history_dir  # Use command line argument

    if cdp_url:
        print(f"üîß CDP Endpoint: {cdp_url}")
    print(
        f"üîß Headless Mode: {headless} {'(invisible chaos)' if headless else '(visible mayhem)'}"
    )
    print(f"üîß Browser Window: {window_width}x{window_height}")
    if browser_executable:
        print(f"üîß Browser Binary: {browser_executable}")
    print(f"üîß History Archive: {save_history_path}")

    # Create the output directory (because we're organized anarchists)
    os.makedirs(save_history_path, exist_ok=True)
    print(f"üìÅ Created/verified history directory: {save_history_path}")

    # Generate timestamp and unique ID (for proper chaos organization)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    chaos_task_id = str(uuid.uuid4())[:8]  # Shorter UUID for cleaner names
    gif_path = os.path.join(
        save_history_path, f"uzdabrazor_{timestamp}_{chaos_task_id}.gif"
    )
    history_path = os.path.join(
        save_history_path, f"uzdabrazor_{timestamp}_{chaos_task_id}.json"
    )

    print(f"üìÅ GIF Output: {gif_path}")
    print(f"üìÅ JSON History: {history_path}")

    # ü§ñ CREATE THE LLM CLUSTERFUCK ü§ñ
    try:
        print("üèóÔ∏è Firing up the AI chaos engines...")
        main_llm = create_llm_clusterfuck(provider, model)
        print(f"‚úÖ Main LLM clusterfuck locked and loaded: {provider}/{model}")

        # Create extraction LLM (might be the same, might be different, chaos is flexible)
        if extraction_provider == provider and extraction_model == model:
            extraction_llm = main_llm  # Same instance because efficiency
            print(f"‚úÖ Using same LLM for extraction (because we're efficient motherfuckers)")
        else:
            extraction_llm = create_llm_clusterfuck(
                extraction_provider, extraction_model
            )
            print(
                f"‚úÖ Extraction LLM clusterfuck locked and loaded: {extraction_provider}/{extraction_model}"
            )

    except Exception as e:
        print(f"‚ùå CLUSTERFUCK ALERT: Failed to create LLMs: {e}")
        print(
            "   Check your API keys, endpoints, and whether your dikciz smells like smegma."
        )
        print("   üí® This failure was more disappointing than a wet shart in white pants.")
        return

    # üåê CREATE BROWSER SESSION (the chaos delivery vehicle)
    print("üåê Spinning up the browser chaos delivery system...")
    browser_profile = BrowserProfile(
        stealth=stealth_mode,
        headless=headless,
        disable_security=disable_security,
        user_data_dir=user_data_dir if user_data_dir else None,
        executable_path=browser_executable if browser_executable else None,
        window_size={"width": window_width, "height": window_height},
    )

    browser_session = BrowserSession(
        cdp_url=cdp_url if cdp_url else None, browser_profile=browser_profile
    )
    print("‚úÖ Browser chaos delivery system is fucking ready to go")

    print("üöÄ LAUNCHING THE BEAUTIFUL CHAOS...")
    print("=" * 60)

    # ü§ñ CREATE AND RUN THE CHAOTIC AGENT ü§ñ
    agent = Agent(
        task=args.task,
        llm=main_llm,
        page_extraction_llm=extraction_llm,
        browser_session=browser_session,
        generate_gif=gif_path,
        use_vision=use_vision,
        vision_detail_level=vision_detail_level,
    )

    try:
        # üé¨ THE MAIN EVENT - RUN THE CHAOS üé¨
        print(f"üéØ Executing task: {args.task}")
        result = await agent.run()
        print("=" * 60)
        print(f"üéâ CHAOS COMPLETE! Agent result:")
        print(f"üìÑ {result}")

        # üíæ SAVE THE EVIDENCE OF CHAOS
        print(f"üíæ Saving this beautiful clusterfuck for posterity...")
        agent.save_history(history_path)

        # üìä VERIFY THE CHAOS ARTIFACTS
        if os.path.exists(gif_path):
            gif_size = os.path.getsize(gif_path)
            print(f"‚úÖ GIF Evidence: {gif_path} ({gif_size} bytes of pure chaos)")
        else:
            print("‚ö†Ô∏è GIF not found (maybe the chaos was too abstract to record)")

        if os.path.exists(history_path):
            with open(history_path, "r") as f:
                content = f.read()
            print(
                f"‚úÖ History Archive: {history_path} ({len(content)} chars of organized chaos)"
            )
        else:
            print(
                "‚ö†Ô∏è History not found (this shouldn't happen, investigate immediately)"
            )

        print("üè¥‚Äç‚ò†Ô∏è Beautiful chaos execution completed successfully! üè¥‚Äç‚ò†Ô∏è")

    except Exception as e:
        print("=" * 60)
        print(f"üí• CONTROLLED EXPLOSION: Agent chaos failed: {e}")
        print(
            "   (This shit happens when your code smells like dikciz smegma - that's why we have backups)"
        )
        print("   üí® Well that was unexpected... like a shart during a job interview.")

        try:
            agent.save_history(history_path)
            print(f"üíæ Emergency chaos archive saved: {history_path}")
        except Exception as save_error:
            print(f"üíÄ Even the emergency save fucked up: {save_error}")
            print("   (Now that's what I call a complete clusterfuck)")

    finally:
        # üßπ CLEANUP THE CHAOS (responsible anarchists clean up after themselves)
        try:
            await browser_session.close()
            print("üßπ Browser session cleaned up (responsible chaos)")
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è Browser cleanup failed: {cleanup_error}")
            print("   (Browser might be stuck in digital purgatory)")


if __name__ == "__main__":
    print("üè¥‚Äç‚ò†Ô∏è uzdabrazor - The Anal-King of AI Browser Automation That Actually Fucking Works üè¥‚Äç‚ò†Ô∏è")
    print("=" * 80)
    print("Love it or hate it, this shit just works. Deal with it.")
    print("=" * 80)
    asyncio.run(main())
